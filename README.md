## MultilayerPerceptron

MLP utilizes a supervised learning technique called backpropagation for training.
Its multiple layers and non-linear activation distinguish MLP from a linear perceptron. 
It can distinguish data that is not linearly separable.

## Usage
#learn rate (eta) = 0.001
```bash
make
./Pmc 0.001
```
